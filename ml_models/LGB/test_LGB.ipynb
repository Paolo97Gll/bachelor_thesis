{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL) solvers for sklearn enabled: https://intelpython.github.io/daal4py/sklearn.html\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#sns.set_context('paper')\n",
    "\n",
    "# Machine Learning\n",
    "# Model\n",
    "import lightgbm as lgb\n",
    "# Splitter Functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Other\n",
    "import requests\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cycle = True\n",
    "with pd.HDFStore('../../classification/ris/OUT-classified-merged.h5', mode='r') as in_data:\n",
    "    data = pd.DataFrame(in_data['GLITCH'].to_numpy().sort(axis=1))\n",
    "    data = pd.concat((data, pd.DataFrame(in_data['MULTI_GLITCH'].to_numpy().sort(axis=1))))\n",
    "    data = pd.concat((data, pd.DataFrame(in_data['NO_GLITCH'].to_numpy().sort(axis=1))))\n",
    "    \n",
    "    target = np.ones(len(in_data['GLITCH'].to_numpy()))\n",
    "    target = np.concatenate((target, np.ones(len(in_data['MULTI_GLITCH'].to_numpy()))))\n",
    "    target = np.concatenate((target, np.zeros(len(in_data['NO_GLITCH'].to_numpy()))))\n",
    "    #target = pd.DataFrame(target)\n",
    "    \n",
    "    #data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cycle = True\n",
    "with pd.HDFStore('../../classification/ris/OUT-classified-merged.h5', mode='r') as in_data:\n",
    "    for group in ['GLITCH', 'NO_GLITCH']:\n",
    "        if first_cycle == True:\n",
    "            data = np.array(in_data[group].to_numpy())\n",
    "            if group == 'GLITCH':\n",
    "                target = np.ones(len(data))\n",
    "            elif group == 'NO_GLITCH':\n",
    "                target = np.zeros(len(data))\n",
    "            else:\n",
    "                print(\"ERROR.\")\n",
    "            first_cycle = False\n",
    "        else:\n",
    "            data = np.concatenate((data, in_data[group].to_numpy()))\n",
    "            if group == 'GLITCH':\n",
    "                target = np.concatenate((target, np.ones(len(in_data[group].to_numpy()))))\n",
    "            elif group == 'NO_GLITCH':\n",
    "                target = np.concatenate((target, np.zeros(len(in_data[group].to_numpy()))))\n",
    "            else:\n",
    "                print(\"ERROR.\")\n",
    "    data = np.concatenate((data, in_data['MULTI_GLITCH'].to_numpy()))\n",
    "    target = np.concatenate((target, np.ones(len(in_data['MULTI_GLITCH'].to_numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid parameters passed: {'stratified': True}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bc1caa883b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/idp/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid parameters passed: {'stratified': True}"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "y_test = test['target']\n",
    "X_train = train.drop('target', axis=1)\n",
    "X_test = test.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 52,\n",
    "    'metric': ['xentropy', 'l1'],\n",
    "    'verbose': -1,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'learning_rate': 0.0481\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/idp/lib/python3.6/site-packages/lightgbm/engine.py:123: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\ttraining's l1: 0.393418\ttraining's xentropy: 0.504425\tvalid_1's l1: 0.443157\tvalid_1's xentropy: 0.597642\n",
      "[20]\ttraining's l1: 0.31133\ttraining's xentropy: 0.380326\tvalid_1's l1: 0.396619\tvalid_1's xentropy: 0.531568\n",
      "[30]\ttraining's l1: 0.248048\ttraining's xentropy: 0.293117\tvalid_1's l1: 0.360697\tvalid_1's xentropy: 0.484452\n",
      "[40]\ttraining's l1: 0.198756\ttraining's xentropy: 0.228815\tvalid_1's l1: 0.333823\tvalid_1's xentropy: 0.447977\n",
      "[50]\ttraining's l1: 0.162952\ttraining's xentropy: 0.182404\tvalid_1's l1: 0.317318\tvalid_1's xentropy: 0.423791\n",
      "[60]\ttraining's l1: 0.136022\ttraining's xentropy: 0.147412\tvalid_1's l1: 0.30798\tvalid_1's xentropy: 0.408028\n",
      "[70]\ttraining's l1: 0.115851\ttraining's xentropy: 0.121505\tvalid_1's l1: 0.300253\tvalid_1's xentropy: 0.394839\n",
      "[80]\ttraining's l1: 0.0990155\ttraining's xentropy: 0.100104\tvalid_1's l1: 0.294926\tvalid_1's xentropy: 0.386119\n",
      "[90]\ttraining's l1: 0.0852536\ttraining's xentropy: 0.0831392\tvalid_1's l1: 0.290307\tvalid_1's xentropy: 0.377001\n",
      "[100]\ttraining's l1: 0.0740917\ttraining's xentropy: 0.0698382\tvalid_1's l1: 0.286794\tvalid_1's xentropy: 0.369864\n",
      "[110]\ttraining's l1: 0.0647811\ttraining's xentropy: 0.0589456\tvalid_1's l1: 0.283889\tvalid_1's xentropy: 0.364718\n",
      "[120]\ttraining's l1: 0.0569159\ttraining's xentropy: 0.0501066\tvalid_1's l1: 0.282532\tvalid_1's xentropy: 0.362556\n",
      "[130]\ttraining's l1: 0.0499915\ttraining's xentropy: 0.0423771\tvalid_1's l1: 0.281198\tvalid_1's xentropy: 0.360249\n",
      "[140]\ttraining's l1: 0.0441567\ttraining's xentropy: 0.0363465\tvalid_1's l1: 0.280112\tvalid_1's xentropy: 0.358426\n",
      "[150]\ttraining's l1: 0.0390274\ttraining's xentropy: 0.0313096\tvalid_1's l1: 0.278527\tvalid_1's xentropy: 0.355753\n",
      "[160]\ttraining's l1: 0.0344743\ttraining's xentropy: 0.0269622\tvalid_1's l1: 0.277596\tvalid_1's xentropy: 0.353768\n",
      "[170]\ttraining's l1: 0.030509\ttraining's xentropy: 0.0232482\tvalid_1's l1: 0.27693\tvalid_1's xentropy: 0.352548\n",
      "[180]\ttraining's l1: 0.0271527\ttraining's xentropy: 0.0200555\tvalid_1's l1: 0.276408\tvalid_1's xentropy: 0.35158\n",
      "[190]\ttraining's l1: 0.0242403\ttraining's xentropy: 0.0172927\tvalid_1's l1: 0.275759\tvalid_1's xentropy: 0.350693\n",
      "[200]\ttraining's l1: 0.0217058\ttraining's xentropy: 0.0149824\tvalid_1's l1: 0.275337\tvalid_1's xentropy: 0.350197\n",
      "[210]\ttraining's l1: 0.0194303\ttraining's xentropy: 0.0131043\tvalid_1's l1: 0.275101\tvalid_1's xentropy: 0.349741\n",
      "[220]\ttraining's l1: 0.0173865\ttraining's xentropy: 0.0114744\tvalid_1's l1: 0.27508\tvalid_1's xentropy: 0.349498\n",
      "[230]\ttraining's l1: 0.0155482\ttraining's xentropy: 0.0101537\tvalid_1's l1: 0.274936\tvalid_1's xentropy: 0.349203\n",
      "[240]\ttraining's l1: 0.0139461\ttraining's xentropy: 0.00899246\tvalid_1's l1: 0.274797\tvalid_1's xentropy: 0.348931\n",
      "[250]\ttraining's l1: 0.0125139\ttraining's xentropy: 0.00794105\tvalid_1's l1: 0.274651\tvalid_1's xentropy: 0.348822\n",
      "[260]\ttraining's l1: 0.011264\ttraining's xentropy: 0.00705852\tvalid_1's l1: 0.274547\tvalid_1's xentropy: 0.34862\n",
      "[270]\ttraining's l1: 0.0101013\ttraining's xentropy: 0.00623684\tvalid_1's l1: 0.274582\tvalid_1's xentropy: 0.348633\n",
      "[280]\ttraining's l1: 0.00906951\ttraining's xentropy: 0.00555324\tvalid_1's l1: 0.274577\tvalid_1's xentropy: 0.348649\n",
      "[290]\ttraining's l1: 0.0081223\ttraining's xentropy: 0.00491277\tvalid_1's l1: 0.274523\tvalid_1's xentropy: 0.34856\n",
      "[300]\ttraining's l1: 0.00732413\ttraining's xentropy: 0.0043902\tvalid_1's l1: 0.274509\tvalid_1's xentropy: 0.348586\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's l1: 0.0118626\ttraining's xentropy: 0.00745591\tvalid_1's l1: 0.274504\tvalid_1's xentropy: 0.348505\n"
     ]
    }
   ],
   "source": [
    "evals_result = {}  # to record eval results for plotting\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[lgb_train, lgb_test],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e5c5e73c7d41e1b7f0e3b19f91d591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='metric_name', options=('xentropy', 'l1'), value='xentropy'), Outpu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_metric(metric_name):\n",
    "    ax = lgb.plot_metric(evals_result, metric=metric_name, figsize=(10, 5))\n",
    "    plt.show()\n",
    "\n",
    "try:\n",
    "    # To enable interactive mode you should install ipywidgets\n",
    "    # https://github.com/jupyter-widgets/ipywidgets\n",
    "    from ipywidgets import interact, SelectMultiple\n",
    "    INTERACTIVE = True\n",
    "except ImportError:\n",
    "    INTERACTIVE = False\n",
    "\n",
    "if INTERACTIVE:\n",
    "    # create widget to switch between metrics\n",
    "    interact(render_metric, metric_name=params['metric'])\n",
    "else:\n",
    "    render_metric(params['metric'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ris = gbm.predict(X_test)\n",
    "ris[ris>=0.5] = 1\n",
    "ris[ris<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/idp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test['ris'] = ris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['ris'] == test['target']].shape[0]/test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629    False\n",
       "585     True\n",
       "25      True\n",
       "321     True\n",
       "418    False\n",
       "       ...  \n",
       "94      True\n",
       "8      False\n",
       "588    False\n",
       "231     True\n",
       "326    False\n",
       "Length: 400, dtype: bool"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_train_aug, y_train_aug = X_train, y_train\n",
    "X_train_aug = np.concatenate((X_train_aug, -X_train))\n",
    "y_train_aug = np.concatenate((y_train_aug, y_train))\n",
    "for j in range(1,100):\n",
    "    X_train_aug = np.concatenate((X_train_aug, np.roll(X_train, j, axis=1)))\n",
    "    X_train_aug = np.concatenate((X_train_aug, -np.roll(X_train, j, axis=1)))\n",
    "    y_train_aug = np.concatenate((y_train_aug, y_train))\n",
    "    y_train_aug = np.concatenate((y_train_aug, y_train))\n",
    "X_train\n",
    "# Apply data augmentation on testing data\n",
    "X_test_aug, y_test_aug = X_test, y_test\n",
    "X_test_aug = np.concatenate((X_test_aug, -X_test))\n",
    "y_test_aug = np.concatenate((y_test_aug, y_test))\n",
    "for j in range(1,100):\n",
    "    X_test_aug = np.concatenate((X_test_aug, np.roll(X_test, j, axis=1)))\n",
    "    X_test_aug = np.concatenate((X_test_aug, -np.roll(X_test, j, axis=1)))\n",
    "    y_test_aug = np.concatenate((y_test_aug, y_test))\n",
    "    y_test_aug = np.concatenate((y_test_aug, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "\n",
    "\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    cv_results = lgb.cv(params, lgb_train, nfold = n_folds, num_boost_round = 10000, early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "  \n",
    "    # Extract the best score\n",
    "    best_score = max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'num_leaves': 2+hp.randint('num_leaves', 150),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "tpe_algorithm = tpe.suggest\n",
    "bayes_trials = Trials()\n",
    "MAX_EVALS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:48<00:00, 21.77s/it, best loss: 0.0001258009063918042]\n",
      "{'learning_rate': 0.04815461602168385, 'num_leaves': 53}\n"
     ]
    }
   ],
   "source": [
    "#best=[]\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "tpe_algorithm = tpe.suggest\n",
    "bayes_trials = Trials()\n",
    "\n",
    "b = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n",
    "print(b)\n",
    "#best.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
