{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFC summary notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#sns.set_context('paper')\n",
    "\n",
    "# Machine Learning\n",
    "# Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Splitter Classes\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# Splitter Functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "# Model validation\n",
    "from sklearn.model_selection import learning_curve\n",
    "# Training metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `print_confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    columnwidth = max([len(str(x)) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    \n",
    "    # Begin CHANGES\n",
    "    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"t\\p\" + (columnwidth-3)//2 * \" \"\n",
    "    \n",
    "    if len(fst_empty_cell) < len(empty_cell):\n",
    "        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n",
    "    # Print header\n",
    "    print(\"    \" + fst_empty_cell, end=\" \")\n",
    "    # End CHANGES\n",
    "    \n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "        \n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `plot_confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels, title, filename, normalize=True, save=True):\n",
    "    # Create DataFrame\n",
    "    df_cm = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "    df_cm.index.name = 'True label'\n",
    "    df_cm.columns.name = 'Predicted label'\n",
    "    \n",
    "    # Normalize\n",
    "    if normalize:\n",
    "        df_cm = df_cm.div(df_cm.sum(axis=1), axis=0).round(decimals=2)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.heatmap(df_cm, cmap='Blues', annot=True)\n",
    "    axlim = ax.get_ylim()\n",
    "    ax.set_ylim(axlim[0] + 0.5, axlim[1] - 0.5)\n",
    "    if normalize:\n",
    "        ax.set_title(title + ' (normalized)')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    if save == True:\n",
    "        fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `plot_learning_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, title, filename, ylim=None, xscale=None, cv=None,\n",
    "                        n_jobs=-1, n_curve_steps=20, save=True):\n",
    "    train_sizes = np.linspace(.1, 1.0, n_curve_steps)\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    if xscale is not None:\n",
    "        plt.xscale(xscale)\n",
    "    plt.xlabel('Training dataset size')\n",
    "    plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color='r')\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "    plt.plot(train_sizes, train_scores_mean, linestyle='--', color='r',\n",
    "             label='Training score')\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color='g',\n",
    "             label='Cross-validation score')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `plot_roc_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(estimator, X, y, cv, title, filename, save=True):\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    i = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.3f)' % (i, roc_auc))\n",
    "        i += 1\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if save == True:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "Load data and target from `classification/ris/OUT-classified-merged.h5` and load into numpy arrays.\n",
    "- **Label `0` = NO GLITCH**\n",
    "- **Label `1` = GLITCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "first_cycle = True\n",
    "with pd.HDFStore('../../classification/ris/OUT-classified-merged.h5', mode='r') as in_data:\n",
    "    for group in ['GLITCH', 'NO_GLITCH']:\n",
    "        if first_cycle == True:\n",
    "            data = np.array(in_data[group].to_numpy())\n",
    "            if group == 'GLITCH':\n",
    "                target = np.ones(len(data))\n",
    "            elif group == 'NO_GLITCH':\n",
    "                target = np.zeros(len(data))\n",
    "            else:\n",
    "                print(\"ERROR.\")\n",
    "            first_cycle = False\n",
    "        else:\n",
    "            data = np.concatenate((data, in_data[group].to_numpy()))\n",
    "            if group == 'GLITCH':\n",
    "                target = np.concatenate((target, np.ones(len(in_data[group].to_numpy()))))\n",
    "            elif group == 'NO_GLITCH':\n",
    "                target = np.concatenate((target, np.zeros(len(in_data[group].to_numpy()))))\n",
    "            else:\n",
    "                print(\"ERROR.\")\n",
    "    data = np.concatenate((data, in_data['MULTI_GLITCH'].to_numpy()))\n",
    "    target = np.concatenate((target, np.ones(len(in_data['MULTI_GLITCH'].to_numpy()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard training\n",
    "\n",
    "Initialize best hyper-parameters founded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bootstrap = False\n",
    "best_max_depth = None\n",
    "best_max_features = 'sqrt'\n",
    "best_min_samples_leaf = 1\n",
    "best_min_samples_split = 3\n",
    "best_n_estimators = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Use k-fold to make a cross validation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# k-fold parameters\n",
    "n_splits = 5\n",
    "n_repeats = 6\n",
    "\n",
    "# Stratified k-fold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=None)\n",
    "scores = np.array([])\n",
    "# Make k-fold CV\n",
    "for train_index, test_index in rskf.split(data, target):\n",
    "    clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                                 max_depth=best_max_depth,\n",
    "                                 max_features=best_max_features,\n",
    "                                 min_samples_leaf=best_min_samples_leaf,\n",
    "                                 min_samples_split=best_min_samples_split,\n",
    "                                 n_estimators=best_n_estimators,\n",
    "                                 n_jobs=-1)\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_test, y_test)\n",
    "    scores = np.append(scores, train_score)\n",
    "    \n",
    "# Print final score\n",
    "print('Score:', scores.mean(), '+-', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix, learning curve and ROC curve\n",
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                             max_depth=best_max_depth,\n",
    "                             max_features=best_max_features,\n",
    "                             min_samples_leaf=best_min_samples_leaf,\n",
    "                             min_samples_split=best_min_samples_split,\n",
    "                             n_estimators=best_n_estimators,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=None, stratify=target)\n",
    "# Train and predict\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "labels = [0., 1.]\n",
    "labels_text = ['no glitch', 'glitch']\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "# Print confusion matrix\n",
    "print('Text confusion matrix (without normalization)\\n')\n",
    "print_confusion_matrix(cm, labels_text)\n",
    "print('\\n\\nGraphic confusion matrix (normalized)\\n')\n",
    "plot_confusion_matrix(cm, labels_text, 'Confusion matrix (with multi glitch)', 'ris/plots/confusion_matrix.pdf', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier and cross-validation method\n",
    "clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                             max_depth=best_max_depth,\n",
    "                             max_features=best_max_features,\n",
    "                             min_samples_leaf=best_min_samples_leaf,\n",
    "                             min_samples_split=best_min_samples_split,\n",
    "                             n_estimators=best_n_estimators,\n",
    "                             n_jobs=-1)\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=0)\n",
    "\n",
    "# Plot learning curve\n",
    "plot_learning_curve(clf, data, target, 'Learning curve (with multi glitch)', 'ris/plots/learning_curve.pdf', cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier and cross-validation method\n",
    "clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                             max_depth=best_max_depth,\n",
    "                             max_features=best_max_features,\n",
    "                             min_samples_leaf=best_min_samples_leaf,\n",
    "                             min_samples_split=best_min_samples_split,\n",
    "                             n_estimators=best_n_estimators,\n",
    "                             n_jobs=-1)\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(clf, data, target, cv, 'ROC curve (with multi glitch)', 'ris/plots/roc_curve.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "Data augmentation is a strategy that increase the diversity of data available for training models, without actually collecting new data. The data augmentation techniques used in this case are vertical flipping and translations.\n",
    "\n",
    "**N.B.: the training time is extremely big!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Due to the long training time, this part will be made in another script on `belen`.\n",
    "\n",
    "### Simple training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# See data_augmentation-score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier training\n",
    "\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See data_augmentation-score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix, learning curve and ROC curve\n",
    "\n",
    "Due to the long training time, this part will be made in another script on `belen`, called `data_augmentation_plots.py` .\n",
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T09:32:39.445374Z",
     "start_time": "2019-09-28T09:32:39.442706Z"
    }
   },
   "source": [
    "# Sorted data\n",
    "\n",
    "Glitches data have a peak, but it is in a random point of the plot. We can help the model by sorting data values: the new features queue will be about the same for all the data, while the first features will became discriminant. _In this case, data augmentation technique have no sense._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.sort(data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize best hyper-parameters founded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bootstrap = False\n",
    "best_max_depth = 40\n",
    "best_max_features = 'sqrt'\n",
    "best_min_samples_leaf = 1\n",
    "best_min_samples_split = 2\n",
    "best_n_estimators = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Use k-fold to make a cross validation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "# k-fold parameters\n",
    "n_splits = 5\n",
    "n_repeats = 6\n",
    "\n",
    "# Stratified k-fold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=None)\n",
    "scores = np.array([])\n",
    "# Make k-fold CV\n",
    "for train_index, test_index in rskf.split(data_s, target):\n",
    "    clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                                 max_depth=best_max_depth,\n",
    "                                 max_features=best_max_features,\n",
    "                                 min_samples_leaf=best_min_samples_leaf,\n",
    "                                 min_samples_split=best_min_samples_split,\n",
    "                                 n_estimators=best_n_estimators,\n",
    "                                 n_jobs=-1)\n",
    "    X_train, X_test = data_s[train_index], data_s[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_test, y_test)\n",
    "    scores = np.append(scores, train_score)\n",
    "    \n",
    "# Print final score\n",
    "print('Score:', scores.mean(), '+-', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix, learning curve and ROC curve\n",
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                             max_depth=best_max_depth,\n",
    "                             max_features=best_max_features,\n",
    "                             min_samples_leaf=best_min_samples_leaf,\n",
    "                             min_samples_split=best_min_samples_split,\n",
    "                             n_estimators=best_n_estimators,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_s, target, test_size=0.3, random_state=0, stratify=target)\n",
    "# Train and predict\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "labels = [0., 1.]\n",
    "labels_text = ['no glitch', 'glitch']\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "# Print confusion matrix\n",
    "print('Text confusion matrix (without normalization)\\n')\n",
    "print_confusion_matrix(cm, labels_text)\n",
    "print('\\n\\nGraphic confusion matrix (normalized)\\n')\n",
    "plot_confusion_matrix(cm, labels_text, 'Confusion matrix (with multi glitch) (sorted)', 'ris/plots/s-confusion_matrix.pdf', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier and cross-validation method\n",
    "clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                             max_depth=best_max_depth,\n",
    "                             max_features=best_max_features,\n",
    "                             min_samples_leaf=best_min_samples_leaf,\n",
    "                             min_samples_split=best_min_samples_split,\n",
    "                             n_estimators=best_n_estimators,\n",
    "                             n_jobs=-1)\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=0)\n",
    "\n",
    "# Plot learning curve\n",
    "plot_learning_curve(clf, data_s, target, 'Learning curve (with multi glitch) (sorted)', 'ris/plots/s-learning_curve.pdf', cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier and cross-validation method\n",
    "clf = RandomForestClassifier(bootstrap=best_bootstrap,\n",
    "                             max_depth=best_max_depth,\n",
    "                             max_features=best_max_features,\n",
    "                             min_samples_leaf=best_min_samples_leaf,\n",
    "                             min_samples_split=best_min_samples_split,\n",
    "                             n_estimators=best_n_estimators,\n",
    "                             n_jobs=-1)\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(clf, data_s, target, cv, 'ROC curve (with multi glitch) (sorted)', 'ris/plots/s-roc_curve.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "465px",
    "width": "403px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
